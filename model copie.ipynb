{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437bea88-36ca-4e30-8983-8f378096b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Import DataSets\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv('data_train.csv')\n",
    "y_test = pd.read_csv('test_competition.csv')\n",
    "\n",
    "\n",
    "Y_train = df[\"default.payment.next.month\"]\n",
    "\n",
    "X_train = df.drop(columns = [\"default.payment.next.month\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54085a7-0d5c-4044-8b74-c5ef156492a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib \n",
    "\n",
    "# #Splitting DataFrame\n",
    "# X_train = df.iloc[:20000,:]\n",
    "\n",
    "# Y_train = X_train[\"default.payment.next.month\"]\n",
    "\n",
    "# X_train = X_train.drop(columns = [\"default.payment.next.month\"])\n",
    "\n",
    "\n",
    "\n",
    "# y_test = df.iloc[20000:,:]\n",
    "\n",
    "# y_test.to_csv(\"test_5000.csv\", index=False)\n",
    "\n",
    "# y_test = y_test.drop(columns = [\"default.payment.next.month\"])\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# # # \n",
    "# # y_test = df.iloc[20000:,:]\n",
    "# # y_test = y_test.drop(columns = [\"default.payment.next.month\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc61613-afa1-4d9b-a006-a8e71999912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using smote to overSample minorities in the dataset for avoiding bias\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# print(\"Smoting...\")\n",
    "\n",
    "# # Step 2: Apply SMOTE to the training data\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# print(\"Data over sampled !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8aee1d-5919-40aa-9fd1-17393adc6100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RandomForestRegressor...\n",
      "Loading GridSearchCV model...\n",
      "Fitting GridSearchCV model...\n",
      "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "1080 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "427 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "653 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.22224265 0.22271652 0.22293422\n",
      " 0.2216247  0.22228425 0.22241547 0.22184976 0.22270759 0.22268229\n",
      " 0.22096458 0.22231916 0.22257256 0.22187628 0.22232216 0.22264287\n",
      " 0.22213287 0.2227217  0.22278677 0.22135588 0.22240054 0.22252345\n",
      " 0.22135588 0.22240054 0.22252345 0.2219067  0.22256903 0.22270403\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.21961664 0.22089862 0.22122889\n",
      " 0.22078167 0.22206032 0.22234128 0.22073482 0.22125437 0.22167187\n",
      " 0.22060603 0.22102219 0.22157395 0.22080794 0.2211873  0.22166182\n",
      " 0.22064979 0.2215273  0.22185318 0.22158617 0.22218161 0.2226336\n",
      " 0.22158617 0.22218161 0.2226336  0.22061516 0.22149091 0.2223606\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.21460361 0.2159791  0.21667136\n",
      " 0.21336949 0.21505908 0.21621338 0.21527848 0.21691114 0.21718444\n",
      " 0.21673037 0.21772643 0.21860104 0.21537756 0.21668923 0.21774715\n",
      " 0.21744389 0.21913019 0.21949344 0.21792264 0.21899553 0.21959518\n",
      " 0.21792264 0.21899553 0.21959518 0.21959205 0.2208315  0.22117458\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.20862671 0.21120158 0.21185942\n",
      " 0.20861801 0.21141856 0.21268987 0.21337447 0.21457787 0.2157878\n",
      " 0.21321081 0.21490848 0.21564386 0.21189603 0.21480306 0.21587519\n",
      " 0.2155181  0.21651375 0.21727277 0.21737459 0.21845949 0.21833112\n",
      " 0.21737459 0.21845949 0.21833112 0.21863564 0.21988969 0.22053877]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV fitted\n",
      "\n",
      "Best parameters found:\n",
      "max_depth: 10\n",
      "max_features: sqrt\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 2\n",
      "n_estimators: 200\n",
      "\n",
      "Best r2 score: 0.2229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def bestModelFindingWithRandomForest(param_grid, scoring, X_train, Y_train, X_train_reshape=False, Y_train_reshape=False):\n",
    "    \"\"\"\n",
    "    Find best Random Forest model using GridSearchCV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    param_grid : dict\n",
    "        Parameter grid for GridSearchCV\n",
    "    scoring : str\n",
    "        Scoring metric ('r2', 'neg_mean_squared_error', etc.)\n",
    "    X_train : array-like\n",
    "        Training features\n",
    "    Y_train : array-like\n",
    "        Training labels\n",
    "    X_train_reshape : bool\n",
    "        Whether to reshape X_train\n",
    "    Y_train_reshape : bool\n",
    "        Whether to reshape Y_train\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading RandomForestRegressor...\")\n",
    "        model = RandomForestRegressor(random_state=100)\n",
    "        \n",
    "        if X_train_reshape:\n",
    "            print(\"Reshaping X_train...\")\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "            \n",
    "        if Y_train_reshape:\n",
    "            print(\"Reshaping Y_train...\")\n",
    "            Y_train = Y_train.reshape(Y_train.shape[0], -1)\n",
    "            \n",
    "        print(\"Loading GridSearchCV model...\")\n",
    "        # Using GridSearchCV with 10-fold cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=10,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Fitting GridSearchCV model...\")\n",
    "        # Fit the model to find the best hyperparameters\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "        \n",
    "        print(\"GridSearchCV fitted\")\n",
    "        \n",
    "        # Best parameters and score obtained\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        \n",
    "        # Format score based on metric\n",
    "        if scoring.startswith('neg_'):\n",
    "            best_score = -best_score  # Convert negative metrics back to positive\n",
    "        \n",
    "        print(\"\\nBest parameters found:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        print(f\"\\nBest {scoring} score: {best_score:.4f}\")\n",
    "        \n",
    "        return grid_search.best_estimator_\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Define the parameter grid for regression\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],           # Number of trees\n",
    "    'max_depth': [10, 15, 20, 25],             # Tree depth\n",
    "    'min_samples_split': [2, 5, 10],           # Minimum samples required to split\n",
    "    'min_samples_leaf': [1, 2, 4],             # Minimum samples required in leaf\n",
    "    'max_features': ['auto', 'sqrt']           # Number of features to consider\n",
    "}\n",
    "\n",
    "bestModel = bestModelFindingWithRandomForest(\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',  # or 'neg_mean_squared_error', 'neg_mean_absolute_error'\n",
    "    X_train=X_train,\n",
    "    Y_train=Y_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f08da7-51ed-4514-9798-799ba3b5ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/anaconda3/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7be8528a-8908-46b8-b051-8e8fad615361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lightgbm...\n",
      "Loading GridSearch model...\n",
      "Fitting GridSearch model...\n",
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 76\u001b[0m\n\u001b[1;32m     68\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m600\u001b[39m],\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m , \u001b[38;5;241m0.08\u001b[39m],\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m63\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m120\u001b[39m]\n\u001b[1;32m     73\u001b[0m }\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Find the best model\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m bestModel \u001b[38;5;241m=\u001b[39m bestModelFinding(param_grid, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train, Y_train)\n",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m, in \u001b[0;36mbestModelFinding\u001b[0;34m(param_grid, scoring, X_train, Y_train)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting GridSearch model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Fit the GridSearch model to find the best hyperparameters\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGridSearch fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Best parameters and best score found\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    966\u001b[0m         clone(base_estimator),\n\u001b[1;32m    967\u001b[0m         X,\n\u001b[1;32m    968\u001b[0m         y,\n\u001b[1;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    975\u001b[0m     )\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# METHOD 2\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def bestModelFinding(param_grid, scoring, X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Find the best LightGBM model using GridSearchCV for regression tasks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    param_grid : dict\n",
    "        Parameter grid for GridSearchCV\n",
    "    scoring : str\n",
    "        Scoring metric ('neg_mean_squared_error', 'r2', etc.)\n",
    "    X_train : array-like\n",
    "        Training features\n",
    "    Y_train : array-like\n",
    "        Training labels\n",
    "    \"\"\"\n",
    "    # Base parameters for regression\n",
    "    base_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',  # Root Mean Square Error\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Loading model\n",
    "        print(\"Loading lightgbm...\")\n",
    "        # Initialize the LGBMRegressor with base parameters\n",
    "        model = lgb.LGBMRegressor(**base_params)\n",
    "        \n",
    "        print(\"Loading GridSearch model...\")\n",
    "        # Configure GridSearchCV with 5-fold cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1  # for progress tracking\n",
    "        )\n",
    "        \n",
    "        print(\"Fitting GridSearch model...\")\n",
    "        # Fit the GridSearch model to find the best hyperparameters\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "        print(\"GridSearch fitted\")\n",
    "        \n",
    "        # Best parameters and best score found\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_ * 100  # In percentage \n",
    "\n",
    "        print(\"\\nBest parameters found:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        print(f\"\\nBest {scoring} score: {best_score:.2f}%\")\n",
    "        \n",
    "        return grid_search.best_estimator_\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define your parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200,600],\n",
    "    'learning_rate': [0.01, 0.1 , 0.08],\n",
    "    'max_depth': [2,5, 10, 25,30,90, 100],\n",
    "    'num_leaves': [30,63, 100, 120]\n",
    "}\n",
    "\n",
    "# Find the best model\n",
    "bestModel = bestModelFinding(param_grid, \"neg_mean_squared_error\", X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5468e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0653004  0.08790907 0.5755109  ... 0.37429492 0.76042562 0.25904249]\n",
      "Saved sorted predictions to sorted_df.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "predicted_payment_value = bestModel.predict(y_test)\n",
    "print(predicted_payment_value)\n",
    "\n",
    "# Assuming y_test is a DataFrame containing the 'ID' column\n",
    "predicted_df = pd.DataFrame({\n",
    "    'ID': y_test['ID'],       # Retrieve the 'ID' column from y_test\n",
    "    'default.probability': predicted_payment_value  # Predicted payment values from the model\n",
    "})\n",
    "\n",
    "# Sort predicted_df by the 'PAYED' column in descending order and extract sorted IDs\n",
    "sorted_df = predicted_df.sort_values(by='default.probability', ascending=False)\n",
    "\n",
    "# Display the resulting DataFrame of sorted IDs and save as a CSV\n",
    "sorted_df.head(1000).to_csv(\"sorted_df.csv\", index=False)\n",
    "print(\"Saved sorted predictions to sorted_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7174b-2f8b-44e6-93b0-451eaf7999fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LGBMRegressor' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m predicted_survival \u001b[38;5;241m=\u001b[39m bestModel\u001b[38;5;241m.\u001b[39mpredict_proba(y_test)  \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Extracting the first column of predicted probabilities\u001b[39;00m\n\u001b[1;32m      8\u001b[0m unsortedPredictedPaymentValue \u001b[38;5;241m=\u001b[39m predicted_survival[:, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LGBMRegressor' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Prediction step\n",
    "print(\"Predicting...\")\n",
    "predicted_survival = bestModel.predict_proba(y_test)  \n",
    "\n",
    "# Extracting the first column of predicted probabilities\n",
    "unsortedPredictedPaymentValue = predicted_survival[:, 1]\n",
    "\n",
    "# Combining Passenger ID and predicted payment probabilities into a DataFrame\n",
    "predicted_df = pd.DataFrame({\n",
    "    'ID': y_test['ID'],      # Get the PassengerId from y_test\n",
    "    'PAYED': unsortedPredictedPaymentValue  # Add the predicted payment probability (class 0)\n",
    "})\n",
    "\n",
    "# Sort predicted_df by the 'PAYED' column in descending order and extract sorted IDs\n",
    "sorted_df = predicted_df.sort_values(by='PAYED', ascending=False)[['ID']]\n",
    "\n",
    "# Display the resulting DataFrame of sorted IDs\n",
    "\n",
    "sorted_df.head(1000).to_csv(\"sorted_df.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
